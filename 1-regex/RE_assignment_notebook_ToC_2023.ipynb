{"cells":[{"cell_type":"markdown","metadata":{"id":"4gHPcg59zjr6"},"source":["Theory of Computation 2023 - Horacio Saggion\n","\n","Deliverable Regular Expressions\n","\n","\n","Please indicate the full names and NIAs of the team members as well as the team number\n","\n","TEAM: #10\n","\n","MEMBERS:\n","\n","\t(253885) - Luca Franceschi\n","\t(253048) - Candela Álvarez López\n","\t(254537) - Pau Ametller López"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGcudUS8K4Jl"},"outputs":[],"source":["# If using Colaborate then allow Google to use your data files\n","\n","# CHANGE BEFORE DELIVERING\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YA1x6iboL-8o"},"outputs":[],"source":["# CHANGE BEFORE DELIVERING\n","# cd to the directory in drive you will use (change to your shared folder)\n","# %cd /content/drive/Shareddrives/ToC-2023/DELIVS/DELIV-1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize, TweetTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zqQQX_DMHx3"},"outputs":[],"source":["# reading some data using the pandas library\n","\n","# CHANGE BEFORE DELIVERING\n","# my_data=pd.read_csv('DATA/ToC_2023_REs.csv', sep=',')\n","my_data=pd.read_csv('ToC_2023_REs.csv', sep=',')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrZLazXSMs-r"},"outputs":[],"source":["# the data is from twitter so it will contain interesting content\n","# we only need the column 'text' to work with\n","\n","twitter_data=my_data['text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qgurvr6NA7W"},"outputs":[],"source":["# TODO print 10 lines of the data to understand what type of text we are working with\n","\n","display(twitter_data.head(10))"]},{"cell_type":"markdown","metadata":{"id":"pWqVvz4x0t_G"},"source":["TODO: Understanding Regular\n","\n","Using the notation used in theory write regular expressions for\n","\n","\n","1. zero or more 'a'\n","2. one or more 'a'\n","1. starts with a and it is followed by zero or more b's\n","2. a sequence of one or more digits\n","2. two digits followed by a - followed by two digits\n","1. string of a's of odd length\n","2. string of a's of even length\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDJhBF85JwUs"},"outputs":[],"source":["# TODO: Run the following code comment what the program is doing\n","\n","my_text =  ''' \"aaa bbb abcd xyz\n","\n","123 4567\n","\n","\n","2   4 5 7 8 90\n","\n","Abc dEFG XYZ\n","\" '''\n","\n","# TODO: What are we searching for?\n","# Any even combination of uppercase or lowercase characters not including the empty string.\n","\n","my_regex=re.compile(r'([a-zA-Z][a-zA-Z])+')\n","\n","result=my_regex.findall(my_text)\n","\n","\n","\n","print(result)\n","\n","# TODO: What are we searching for?\n","# Any even combination of uppercase letters not including the empty string.\n","\n","my_regex=re.compile(r'([A-Z][A-Z])+')\n","\n","result=my_regex.findall(my_text)\n","\n","print(result)\n","\n","# TODO: What are we searching for?\n","# Any combination of uppercase letters not including the empty string (basically [A-Z]+).\n","\n","my_regex=re.compile(r'([A-Z][A-Z]*)')\n","\n","result=my_regex.findall(my_text)\n","\n","print(result)\n","\n","\n","\n","# TODO: What is the difference with the above?\n","# In this case it only shows first match or none, while in the above cases it matched all ocurrences of the regular expression\n","\n","result = re.search(r'([A-Z][A-Z]*)', my_text)\n","\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"Dpm1qvtjJMP1"},"source":["TODO: Take the tutorial  at [W3Schools on regular expressions in Python](https://www.w3schools.com/python/python_regex.asp)\n","and practice the code."]},{"cell_type":"markdown","metadata":{"id":"bfXmYXThJlNi"},"source":["TODO: Answer the following questions about RE in python\n","\n","\n","\n","1.  What is the purpose of the findall function\n","    Purpose of findall: filter a text and getting only the parts that are meaningful using regular expressions.\n","2.  What is the pupose of the search function\n","    Purpose of search: find if there is a matching expresion or if there is none for that regular expression.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dh6Tz0KTOowX"},"outputs":[],"source":["# TODO: examine item 95 of the data\n","\n","twitter_data.loc[95]"]},{"cell_type":"markdown","metadata":{},"source":["We observe that the item contained is a tweet. In this case composed of some mentions, hashtags, links and text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBZpOMGJPk3n"},"outputs":[],"source":["# TODO: list sentences with string 2030 in the first 100 sentences of your data using a regular expression\n","\n","my_regex = re.compile(r'2030')\n","sentences_2030 = []\n","\n","for item in twitter_data.head(100):\n","    result = my_regex.search(item)\n","    if result != None:\n","        sentences_2030.append(item) #Append the entire sentence\n","\n","print(sentences_2030)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amGGdho6QT9x"},"outputs":[],"source":["# TODO: list sentences with string 2030 in the first 100 sentences of your data\n","# this time 2030 should be a full word not part of a word\n","\n","my_regex = re.compile(r'(\\b2030\\b)')\n","sentences_2030 = []\n","\n","for item in twitter_data.head(100):\n","    result = my_regex.search(item)\n","    if result != None:\n","        sentences_2030.append(item) #Append the entire sentence\n","print(sentences_2030)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9V1euKcIRRx-"},"outputs":[],"source":["# TODO:  find all sentences, within the first 100 sentences,  containing a twitter hash tag '#' using regular expressions\n","\n","my_regex = re.compile(r'#')\n","sentences_hashtag = []\n","\n","for item in twitter_data.head(100):\n","    result = my_regex.search(item)\n","    if result != None:\n","        sentences_hashtag.append(item) #Append the entire sentence\n","print(sentences_hashtag)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijoQCwsYctYe"},"outputs":[],"source":["# install library NLTK to  work with texts\n","\n","# CHANGE BEFORE DELIVERING\n","# !pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7v1f7hMdC1g"},"outputs":[],"source":["# import stopwords and punctuation for English\n","\n","stops=nltk.download('stopwords')\n","punkt=nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3aHDvPamciXx"},"outputs":[],"source":["# word tokenization with nltk\n","\n","text = \"Oh!!! I can't believe it is Friday.\"\n","print(word_tokenize(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZRkwjJddl_W"},"outputs":[],"source":["# TODO tokenizing text vs tokenizing tweets\n","# produce a tokenization using word_tokenize(...) and a tokenization using the tweet tokenizer ( TweetTokenizer() )\n","\n","tweet_tokenizer = TweetTokenizer()\n","print(twitter_data.loc[1]) #Normal text\n","print(word_tokenize(twitter_data.loc[1])) #Word tokenized text\n","print(tweet_tokenizer.tokenize(twitter_data.loc[1])) #Tweet tokenized text"]},{"cell_type":"markdown","metadata":{},"source":["How are they different?\n","The Word tokenize splits all symbols while twitter tokenize keeps mentions, hashtag, abbreviations and links together as one element. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvqeoFnAfVo8"},"outputs":[],"source":["# TODO: what do you get if you tokenize item 95 of the data, show the tokens\n","\n","print(word_tokenize(twitter_data.loc[95]))\n","print(tweet_tokenizer.tokenize(twitter_data.loc[95]))"]},{"cell_type":"markdown","metadata":{},"source":["What we get when we word tokenize item 95 is that it once again separates symbols such as # and @ yet it does not separate emojis (wrongly), meanwhile twitter tokenizes emojis but does not separate hashtags, mentions, abbreviations nor links."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVrJ6wzQRPs2"},"outputs":[],"source":["print(format(ord('☛'), '#08x'))\n","print(format(ord('♀'), '#08x'))\n","print(format(ord('♂'), '#08x'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Co2_FitMhWRs"},"outputs":[],"source":["# TODO: using a regular expression list  the emojis ☛   ♀  or ♂  and the position in which the occur\n","# You should use the UNICODE representation of such emojis, which you can figure out checking a table or using\n","# print(format(ord(CHARACTER), '#08x'))\n","\n","my_regex = re.compile(r'\\u261b|\\u2640|\\u2642')\n","symbols = []\n","positions = []\n","\n","for item in twitter_data:\n","    result = my_regex.search(item)\n","    if result != None:\n","        positions.append(result.start()) #Get starting position of matched item\n","        symbols.append(result.group()) #Append the part of the string where there was a match\n","    \n","print(symbols)\n","print(positions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nav-_y5jb5sf"},"outputs":[],"source":["# TODO: extract all hashtags in first 100 sentences\n","\n","my_regex = re.compile(r'#\\S+') #Returns a match where the string does not contain a white space characther\n","sentences_hashtag = []\n","\n","for item in twitter_data.head(100):\n","    result = my_regex.search(item)\n","    if result != None:\n","        sentences_hashtag.append(result.group()) #Append the part of the string where there was a match\n","\n","print(sentences_hashtag)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmRllXHWTSDj"},"outputs":[],"source":["# TODO: extract only the TAG of the hashtag in the first 100 sentences\n","\n","#IN PROCESS\n","my_regex = re.compile(r'#')\n","sentences_TAG = []\n","\n","for item in twitter_data.head(100):\n","    result = my_regex.search(item)\n","    if result != None:\n","        sentences_TAG.append(result.group()) #Append the part of the string where there was a match\n","\n","print(sentences_TAG)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NdPPFQCf3EM"},"outputs":[],"source":["# TODO: list all hashtags which include a year (something that looks like YYYY)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WipAUhfUCT9"},"outputs":[],"source":["# TODO: for all hashtags including a year, extract the year"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05SpXJ5QoSlD"},"outputs":[],"source":["# TODO:  for all hashtags including a year, extract the year, list each year once"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"694J_vwO4IUF"},"outputs":[],"source":["# TODO: find all user mentions in the first 1000 sentences\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMV6Sbjv4mxL"},"outputs":[],"source":["# TODO: find all user mentions such that contain UPPERCASE letters only, in the first 1000 sentences\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ruZ9sM28tsQ"},"outputs":[],"source":["# TODO: find all strings containing the substring UN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLZ_qxcw9aRP"},"outputs":[],"source":["# TODO: find all strings containing the substring UN strictly in the middle of the token\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx-d8JmW9t-K"},"outputs":[],"source":["# TODO: find all strings containing just numbers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ekoRz_y-CCu"},"outputs":[],"source":["# TODO: find all strings containing no numbers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s42UbWXEDTxI"},"outputs":[],"source":["# TODO: find any string between double quotes in the sentences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQlNBZDSE_R5"},"outputs":[],"source":["# TODO: extract all urls in the 100 first sentences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhi6l_M7HWR5"},"outputs":[],"source":["# TODO: extract all dates (can be DD/MM/YY or DD/MM/YYYY)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0mguuUvi296x"},"outputs":[],"source":["# TODO: consider the following list of UNICODEs for emojis\n","emojis=['\\U0001F603', '\\U0001F604', '\\U0001F601', '\\U0001F606', '\\U0001F605', '\\U0001F923',\n","\t\t'\\U0001F602', '\\U0001F642', '\\U0001F643', '\\U0001F609', '\\U0001F60A', '\\U0001F607']\n","\n","# print them to understand what they are\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dydH4Gcl6hR6"},"outputs":[],"source":["# TODO: use regular expression to find and list any sentence containing at least one of the emojis above\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XnwDAHtXrFo"},"outputs":[],"source":["# TODO: use regular expressions to find and extract all ocurrences of  previous emojis and count how many of each\n","# list the results\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMvup0uQaMRb"},"outputs":[],"source":["# TODO: identify any sentences with 2 emojis in sequence such as 😊😊\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNPXifHNZ6GSjU/wlbJ7DWU","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
