{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNPXifHNZ6GSjU/wlbJ7DWU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Theory of Computation 2023 - Horacio Saggion\n","\n","Deliverable Regular Expressions\n","\n","\n","Please indicate the full names and NIAs of the team members as well as the team number\n","\n","TEAM:\n","\n","MEMBERS:\n","\n"],"metadata":{"id":"4gHPcg59zjr6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGcudUS8K4Jl"},"outputs":[],"source":["# If using Colaborate then allow Google to use your data files\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#cd to the directory in drive you will use (change to your shared folder)\n","%cd /content/drive/Shareddrives/ToC-2023/DELIVS/DELIV-1"],"metadata":{"id":"YA1x6iboL-8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reading some data using the pandas library\n","import pandas as pd\n","\n","my_data=pd.read_csv('DATA/ToC_2023_REs.csv', sep=',')"],"metadata":{"id":"_zqQQX_DMHx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the data is from twitter so it will contain interesting content\n","# we only need the column 'text' to work with\n","twitter_data=my_data['text']"],"metadata":{"id":"PrZLazXSMs-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO print 10 lines of the data to understand what type of text we are working with\n"],"metadata":{"id":"-qgurvr6NA7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TODO: Understanding Regular\n","\n","Using the notation used in theory write regular expressions for\n","\n","\n","1. zero or more 'a'\n","2. one or more 'a'\n","1. starts with a and it is followed by zero or more b's\n","2. a sequence of one or more digits\n","2. two digits followed by a - followed by two digits\n","1. string of a's of odd length\n","2. string of a's of even length\n","\n"],"metadata":{"id":"pWqVvz4x0t_G"}},{"cell_type":"code","source":["# TODO: Run the following code comment what the program is doing\n","\n","import re\n","\n","my_text =  ''' \"aaa bbb abcd xyz\n","\n","123 4567\n","\n","\n","2   4 5 7 8 90\n","\n","Abc dEFG XYZ\n","\" '''\n","\n","# TODO: What are we searching for?\n","\n","my_regex=re.compile(r'([a-zA-Z][a-zA-Z])+')\n","\n","result=my_regex.findall(my_text)\n","\n","\n","\n","print(result)\n","\n","# TODO: What are we searching for?\n","\n","my_regex=re.compile(r'([A-Z][A-Z])+')\n","\n","result=my_regex.findall(my_text)\n","\n","print(result)\n","\n","# TODO: What are we searching for?\n","\n","my_regex=re.compile(r'([A-Z][A-Z]*)')\n","\n","result=my_regex.findall(my_text)\n","\n","print(result)\n","\n","\n","\n","# TODO: What is the difference with the above?\n","\n","result = re.search(r'([A-Z][A-Z]*)', my_text)\n","\n","print(result)"],"metadata":{"id":"yDJhBF85JwUs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TODO: Take the tutorial  at [W3Schools on regular expressions in Python](https://www.w3schools.com/python/python_regex.asp)\n","and practice the code."],"metadata":{"id":"Dpm1qvtjJMP1"}},{"cell_type":"markdown","source":["TODO: Answer the following questions about RE in python\n","\n","\n","\n","1.  What is the purpose of the findall function\n","2.  What is the pupose of the search function\n","\n"],"metadata":{"id":"bfXmYXThJlNi"}},{"cell_type":"code","source":["# TODO: examine item 95 of the data"],"metadata":{"id":"Dh6Tz0KTOowX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: list sentences with string 2030 in the first 100 sentences of your data using a regular expression\n","\n","\n"],"metadata":{"id":"JBZpOMGJPk3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: list sentences with string 2030 in the first 100 sentences of your data\n","# this time 2030 should be a full word not part of a word\n"],"metadata":{"id":"amGGdho6QT9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO:  find all sentences, within the first 100 sentences,  containing  a twitter hash tag '#' using regular expressions\n","\n","\n"],"metadata":{"id":"9V1euKcIRRx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# install library NLTK to  work with texts\n","!pip install nltk"],"metadata":{"id":"ijoQCwsYctYe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import stopwords and punctuation for English\n","import nltk\n","stops=nltk.download('stopwords')\n","punkt=nltk.download('punkt')"],"metadata":{"id":"v7v1f7hMdC1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# word tokenization with nltk\n","\n","from nltk.tokenize import word_tokenize\n","text = \"Oh!!! I can't believe it is Friday.\"\n","print(word_tokenize(text))"],"metadata":{"id":"3aHDvPamciXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO tokenizing text vs tokenizing tweets\n","from nltk.tokenize import TweetTokenizer\n","\n","# produce a tokenization using word_tokenize(...) and a tokenization using the tweet tokenizer ( TweetTokenizer() )\n","\n","# How are they different?"],"metadata":{"id":"rZRkwjJddl_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: what do you get if you tokenize item 95 of the data, show the tokens"],"metadata":{"id":"wvqeoFnAfVo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(format(ord('â˜›'), '#08x'))\n","print(format(ord('â™€'), '#08x'))\n","print(format(ord('â™‚'), '#08x'))"],"metadata":{"id":"qVrJ6wzQRPs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: using a regular expression list  the emojis â˜›   â™€  or â™‚  and the position in which the occur\n","# You should use the UNICODE representation of such emojis, which you can figure out checking a table or using\n","# print(format(ord(CHARACTER), '#08x'))\n","\n"],"metadata":{"id":"Co2_FitMhWRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: extract all hashtags in first 100 sentences\n"],"metadata":{"id":"nav-_y5jb5sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: extract only the TAG of the hashtag in the first 100 sentences\n","\n"],"metadata":{"id":"GmRllXHWTSDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: list all hashtags which include a year (something that looks like YYYY)\n","\n"],"metadata":{"id":"5NdPPFQCf3EM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: for all hashtags including a year, extract the year"],"metadata":{"id":"-WipAUhfUCT9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO:  for all hashtags including a year, extract the year, list each year once"],"metadata":{"id":"05SpXJ5QoSlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: find all user mentions in the first 1000 sentences\n","\n"],"metadata":{"id":"694J_vwO4IUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: find all user mentions such that contain UPPERCASE letters only, in the first 1000 sentences\n","\n"],"metadata":{"id":"pMV6Sbjv4mxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: find all strings containing the substring UN\n"],"metadata":{"id":"2ruZ9sM28tsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: find all strings containing the substring UN strictly in the middle of the token\n"],"metadata":{"id":"dLZ_qxcw9aRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: find all strings containing just numbers\n"],"metadata":{"id":"Nx-d8JmW9t-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: find all strings containing no numbers\n"],"metadata":{"id":"-ekoRz_y-CCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: find any string between double quotes in the sentences\n"],"metadata":{"id":"s42UbWXEDTxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: extract all urls in the 100 first sentences\n"],"metadata":{"id":"ZQlNBZDSE_R5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: extract all dates (can be DD/MM/YY or DD/MM/YYYY)\n"],"metadata":{"id":"nhi6l_M7HWR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: consider the following list of UNICODEs for emojis\n","emojis=['\\U0001F603', '\\U0001F604', '\\U0001F601', '\\U0001F606', '\\U0001F605', '\\U0001F923',\n","\t\t'\\U0001F602', '\\U0001F642', '\\U0001F643', '\\U0001F609', '\\U0001F60A', '\\U0001F607']\n","\n","# print them to understand what they are\n","\n"],"metadata":{"id":"0mguuUvi296x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: use regular expression to find and list any sentence containing at least one of the emojis above\n"],"metadata":{"id":"dydH4Gcl6hR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: use regular expressions to find and extract all ocurrences of  previous emojis and count how many of each\n","# list the results\n","\n"],"metadata":{"id":"6XnwDAHtXrFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: identify any sentences with 2 emojis in sequence such as ðŸ˜ŠðŸ˜Š\n","\n"],"metadata":{"id":"mMvup0uQaMRb"},"execution_count":null,"outputs":[]}]}